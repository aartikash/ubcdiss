\chapter{Challenges}
We face  the following challenges during the design of the technique. 
The  first challenge is choosing a mathematical abstraction technique that works best for our use case. 
The second challenge is mapping the \ac{DNN}  model to the abstraction. 
Finally, the last challenge is designing the \ac{DNN}  for our use case which is attack synthesis. 


\section{Selecting  the automated technique}
The first part is choosing a technique that allows the attacker to model a DNN and find the critical inputs through the modeling. 
There are multiple means of modeling the DNN to synthesize attacks some of which are using SMT solvers, symbolic execution and MILP.
Our goal is twofold: 
\begin{enumerate}
	\item generality
	\item scalability
\end{enumerate}

\ac{APS} has a  feed-forward architecture with 74 inputs whereas the other systems on which we test our technique have completely different architectures as can be observed from Table 6.1 in Chapter 6. 
This is due to the difference in the application domain. APS is a medical system that is much easier to reason about as compared to our other test systems that are collision avoidance systems for air traffic control management.
APS has two layers and is the smallest system that we tested our technique for but we want our technique to be valid for much bigger systems as shown in Table 1. 
We show that MILP fulfills these two criteria by providing us generality and scalability. 


\section{ Mapping  Mixed-Integer Linear Programming Model to Deep Neural Networks}

DNN architecture is quite complex and researchers have used experimental means to understand the security scenarios in the DNN. 
Their  mathematical structure contains multiple layers and the non-linearity introduced due to the activation functions within the network add another layer of complication for modeling in \ac{MILP}.
There are two main complications that arise here:
\begin{enumerate}
	\item Modeling each layer as a set of constraints to build a \ac{MILP} model.
	\ac{APS} has two layers as explained in Chapter 3 and has 50 neurons each. 
	We use that to 
	\item Non-linear terms are not allowed in \ac{MILP} modeling. Hence the big question here is how can we model the activation functions. 
\end{enumerate}

We formalize the \ac{DNN} to represent the entire network as a set of linear constraints. 
We tackle the non-linearity of activation functions by defining them as piece-wise linear which we demonstrate for one activation function ReLU.
We chose ReLU because are three evaluation systems have ReLU in their models. 

\section{Modeling cost function}
The goal of the cost function is to minimize or maximize the linear constraints that represents the \ac{DNN} architecture. 
However, our requirement is different in the traditional sense of using minimization and maximization which is usually used in terms of resource utilization. 
Based on our attack model we want to minimize the input perturbations and maximize the output deviations in a \ac{DNN}.
We do so to find the optimal solutions which we have defined in Section 2.5
There are two main problems that can arise based on our cost function modeling:
\begin{enumerate}
	\item There is a possibility that there are no solutions. 
	This happens if the \ac{MIP} is infeasible and we need to redefine our cost function. 
	\item If the \ac{MIP} is feasible, then there is a possibility that there is no optimal solution since there are infinitely many good cost function values. 
	This makes our solution space unbounded and can lead to state space explosions. 
\end{enumerate}

We demonstrate both the problems in our experimentation and show how we tackle the state space problem that can arise thereby scaling our approach for big sized systems. 
We use interval analysis which we elaborate in Chapter 5.


















