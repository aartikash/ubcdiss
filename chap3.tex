\chapter{Related Work}
\label{ch:Chapter2}

We have classified related work into four broad categories. The first category discusses the verification techniques that have emerged to verify DNNs. We highlight why those techniques cannot directly be used for our application. The second discusses work done in the FDI community for classic control theory modeled systems, and why our work is required to understand the consequences of deploying DNNs in CPS.  In the third category, we discuss multiple applications where DNNs have replaced or have been added to the CPS. This provides a glimpse into how the DNNs are being utilized in safety-critical applications. Furthermore, this motivates the need for a technique that can generalize to different settings of DNN based CPS. Finally, in the last category, we discuss the adversarial attacks that have gained a lot of attention in the Machine Learning(ML) community and how are they different from our \attack. 
\subsection{Verification of CPS}
%Providing formal guarantees for CPS has always been an important part of the literature. The reason is due to their safety-critical nature. 

%\subsection{Control Theory CPS }
%\subsection{DNN based CPS}
Formally verifying DNN based CPS has recently gained a lot of momentum. Researchers have proposed automated verification mechanisms that use techniques involving SMT solvers, symbolic execution and MILP. However, the modeling that has been done for those interfaces cannot be directly used for our use case without significant modifications. The goal of these techniques is to conduct an input-output range analysis to establish bounds for the DNN. Our work focuses not on conducting a bound analysis, but on finding the optimal inputs for changing the outcome of a CPS under a constrained setting for a FDIA. Also, the approaches focus more on verifying certain properties such as reachability analysis that do not account for \attack.

Pulina et al.\cite{10.1007/978-3-642-14295-6_24} proposed one of the first approaches for verifying DNN safety properties using abstraction refinement. They further abstract \cite{article} the nonlinear activation functions in a linear arithmetic SMT formula. They then use counterexample based approach for abstract refinement. There has been further work by Katz et al.\cite{10.1007/978-3-319-63387-9_5} in verifying the networks using SMT solvers where they build rules for handling the ReLU activation function. Most of work we have mentioned above in this domain utilizes the state-of-the art solvers Z3 and CVC4. However, all the existing techniques focus on proving robustness properties \cite{NIPS2016_6339} for DNNs that cannot detect or produce FDIA. Xiang at al.\cite{xiang2017output} focuses on an output reachable estimation for NN using simulation-based methods. There have been multiple methods proposed to conduct a rechability \karthik{Spell check !} analysis for DNN using MILP/SMT approach \cite{10.1145/3302504.3313351} \cite{ehlers2017formal} \cite{10.1007/978-3-319-63387-9_5} \cite{lomuscio2017approach} \cite{article}. Ivanov et al. \cite{ivanov2018verisig} proposes a verification framework for verifying DNN based controllers by abstracting the problem into a hybrid system verification problem.  
%\subsection*{Cyber-Physical Systems Security}
%Sensor spoofing attacks- multiple types such as gps spoofing, most common ones demonstrated are FDI.

\subsection{False Data Injection Attacks (FDIA) in traditional systems}
%FDI attacks in CPS in different domains. Difference in terms of modelling of systems. Sparse FDI attacks
Attacking the CPS by exploiting the deep integration of the physical-nature and cyber- nature of the systems has lead to the emergence of FDIAs. 
There are two main categories of FDI attacks that have been proposed in the literature: random FDIA and targeted FDIA. Random FDIA are generated by randomly changing the inputs to cause changes in the state estimation. Targeted FDIAs are the more interesting ones from an attackers perspective because in this case, they have to synthesize the exact values of inputs to cause a change in the output without triggering alarms. Our work focuses on synthesizing random and targeted FDIAs under specific set of constraints based on the specification documentation. \karthik{I'm surprised to see that we use specification documentation to generate attacks - we never said this so far !}

In FDI attacks, the attacker's goal is to compromise the sensor inputs to mislead the state estimation process \cite{e3f0020abba24d4389aff937fe8bcdd5}. Liu et al. \cite{10.1145/1952982.1952995} introduced the class of FDIA for electric power grids to introduce arbitrary errors into certain state variables without being detected. Giannini et al. \cite{unknown} generate FDIA under a specific setting, %\karthik{What does this mean?}
where they introduce new information as constraints in the state estimator under the assumption that the new information is not available to the attacker. These techniques generate FDIA for systems modeled using control theory in order to cause a wrong state estimation without getting detected. Our work studies the CPS constructed using Deep Neural Networks \karthik{Use abbreviations consistently - we've said DNNs all along, stick to it}. We try to \karthik{Either do or do not. There's no try in formal writing.} understand if FDI attacks would work in different constraint settings. \karthik{Now that you've done the work, you can perhaps say 'we demonstrate'} 
Bobba et al. \cite{Bobba2010DetectingFD} show that by protecting a strategically selected set of sensor measurements, it is possible to detect the attacks proposed by Liu at al. \cite{10.1145/1952982.1952995}.
% Bai et al. \cite{bai2020aigan} proposes AI-GAN which provides a boost up over previous adversarial examples techniques. In contrast, our work is not focusing on adversarial examples for DNNs but FDIA for DNN based CPS in a constrained environment. 

\subsection{DNN controllers in CPS}
This line of work focuses on intelligent control primarily due to the introduction of neural networks(NN) in designing the control systems. %As discussed in the Problem Formulation section NN allow the input-output mapping because of the nodes connecting in a network.
Fukuda et al. \cite{170966} show how NN can be utilized in industrial systems. Cong et al. \cite{Cong} proposes a recurrent neural network that is used to create a PID neural network (PIDNN). This allows the network to converge faster since PIDNN has one hidden layer with 3 neurons that represent K$_p$, K$_i$ and K$_d$ that are used in traditional PID controllers. Wang et al. \cite{Wang2016ACA} proposes a double layer architecture for a non-linear system using adaptive NN control and Nonlinear Model Predictive Control (NMPC).

\subsection{Deep Neural Network Security}
%ML security such as adversarial ML
%adding noise to the inputs to change classification but they are based on RNN and other such networks- we use MILP optimization in order to find attacks that cause slight deviations and do not necessarily change the output
%DNNs are susceptible to attacks as is.

The deep learning security comprises the adversarial examples field which was introduced by Szegedy et al. \cite{Szegedy2013IntriguingPO} where they showed that the input-output mappings for the DNNs are not continuous and it is possible to perturb the inputs by small amounts such that the classification changes. Since then there has been a lot of work exploring different types of adversarial attacks. Deng et al. \cite{deng2020analysis} analyses five types of adversarial attacks in autonomous driving models.  ConAML \cite{li2020conaml} explores the constrained adversarial attacks for cyber-physical systems\karthik{Consistent abbreviations !} under different settings. They propose a best effort algorithm to iteratively generate adversarial examples. We, on the other hand, introduce \attack  that generate ripples based on constraints. Wang et al. \cite{217595} proposes ReLUVal that uses symbolic execution to show that the DNN based systems are free from security vulnerabilities. They use symbolic execution \karthik{You just told me this} to ensure that in specific intervals, the system is free from attacks, whereas we use MILP \karthik{So how's MILP better?}. We test our \tool on APS to find \attack, whereas ReLUVal using the notion of symbolic intervals shows that CPS are free of drift attacks. 
\karthik{Again, I don't understand what the main point of the above sentence is. Is it that we use find attacks while they show the absence of attacks. Your writing is really convoluted !}
%The goal of our work is to find if a DNN based CPS has ripples and to do so we use MILP whose goal is to find a \attack if it exists.
% \karthik{What about the ReLuVal paper [37] ?}

%\aarti{Add more papers here}
%CPS verification 
%Summary
This section covers the various techniques that have been used in verification of DNNs, and explains how none of the \karthik{You really need to proofread your work before sending it to me. It's unacceptable to have incomplete and dangling sentences !}