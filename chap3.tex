\chapter{Related Work}
\label{relatedwork}

We have classified related work into four broad categories. 
The first category discusses the verification techniques that have emerged to verify DNNs. We highlight why those techniques cannot directly be used for our application. The second discusses work done in the \ac{FDIA} community for classic control theory modeled systems, and why our work is required when \ac{DNN}s replace control theory equations. 
In the third category, we discuss multiple applications where DNNs have replaced or have been added to the CPS. This provides a glimpse into how the DNNs are being utilized in safety-critical applications. Furthermore, this motivates the need for a technique that can generalize to different settings of DNN based CPS.
 Finally, in the last category, we discuss adversarial attacks that have gained a lot of attention in the Machine Learning(ML) community and how are they different from our \attack. 
\section{Verification of CPS}
%Providing formal guarantees for CPS has always been an important part of the literature. The reason is due to their safety-critical nature. 

%\subsection{Control Theory CPS }
%\subsection{DNN based CPS}
Formally verifying DNN based CPS has recently gained a lot of momentum. Researchers have proposed automated verification mechanisms that use techniques involving SMT solvers, symbolic execution and MILP \cite{10.1007/978-3-642-14295-6_24}, \cite{article}, \cite{10.1007/978-3-319-63387-9_5}. 
However, the modeling that has been done for those interfaces cannot be directly used for our use case without significant modifications. 
The goal of these techniques is to conduct an input-output range analysis to establish bounds for the DNN for its correctness. Our work focuses not on conducting a bound analysis, but on finding the optimal inputs for changing the outcome of a CPS under a constrained setting for a FDIA. Also, the approaches focus more on verifying certain properties such as reachability analysis that do not account for \attack.

Pulina et al.\cite{10.1007/978-3-642-14295-6_24} proposed one of the first approaches for verifying DNN safety properties using abstraction refinement. They further abstract \cite{article} the nonlinear activation functions in a linear arithmetic SMT formula but their application and abstraction interface is different from ours. They then use counterexample based approach for abstract refinement. There has been further work by Katz et al.\cite{10.1007/978-3-319-63387-9_5} in verifying the networks using SMT solvers where they build rules for handling the ReLU activation function. Most of work we have mentioned above in this domain utilizes the state-of-the art solvers Z3 and CVC4. However, all the existing techniques focus on proving robustness properties \cite{NIPS2016_6339} for DNNs that cannot detect or produce FDIA. Xiang at al.\cite{xiang2017output} focuses on an output reachable estimation for NN using simulation-based methods. There have been multiple methods proposed to conduct a reachability analysis for DNN using MILP/SMT approach \cite{10.1145/3302504.3313351} \cite{ehlers2017formal} \cite{10.1007/978-3-319-63387-9_5} \cite{lomuscio2017approach} \cite{article}. Ivanov et al. \cite{ivanov2018verisig} proposes a verification framework for verifying DNN based controllers by abstracting the problem into a hybrid system verification problem.  


\section{False Data Injection Attacks (FDIA) in traditional systems}
%FDI attacks in CPS in different domains. Difference in terms of modelling of systems. Sparse FDI attacks
Attacking a CPS by exploiting the deep integration of the physical-nature and cyber- nature of the systems has lead to the emergence of FDIAs. 
There are two main categories of FDI attacks that have been proposed in the literature: random FDIA and targeted FDIA. Random FDIA are generated by randomly changing the inputs to cause changes in the state estimation. Targeted FDIAs are the more interesting ones from an attackers perspective because they have to synthesize the exact values of inputs to cause a change in the output without triggering alarms. Our work focuses on synthesizing both random and targeted FDIAs under specific set of constraints based on the specification documentation that we explain in Chapter ~\ref{background}. 

In FDI attacks, the attacker's goal is to compromise the sensor inputs to mislead the state estimation process \cite{e3f0020abba24d4389aff937fe8bcdd5}. Liu et al. \cite{10.1145/1952982.1952995} introduced the class of FDIA for electric power grids to introduce arbitrary errors into certain state variables without being detected. Giannini et al. \cite{unknown} generate FDIA under a specific setting, 
where they introduce new information as constraints in the state estimator under the assumption that the new information is not available to the attacker. These techniques generate FDIA for systems modeled using control theory in order to cause a wrong state estimation without getting detected. Our work studies the CPS constructed using \ac{DNN}s. We demonstrate if  \ac{FDIA} would work in different constraint settings. 
Bobba et al. \cite{Bobba2010DetectingFD} show that by protecting a strategically selected set of sensor measurements, it is possible to detect the attacks proposed by Liu at al. \cite{10.1145/1952982.1952995}.
% Bai et al. \cite{bai2020aigan} proposes AI-GAN which provides a boost up over previous adversarial examples techniques. In contrast, our work is not focusing on adversarial examples for DNNs but FDIA for DNN based CPS in a constrained environment. 

\section{DNN controllers in CPS}
This line of work focuses on intelligent control primarily due to the introduction of neural networks(NN) in designing the control systems. %As discussed in the Problem Formulation section NN allow the input-output mapping because of the nodes connecting in a network.
Fukuda et al. \cite{170966} show how NN can be utilized in industrial systems. Cong et al. \cite{Cong} proposes a recurrent neural network that is used to create a Proportional–Integral–Derivative (PID) neural network (PIDNN). 
This allows the network to converge more quickly since PIDNN has one hidden layer with 3 neurons that represent K$_p$, K$_i$ and K$_d$ that are used in traditional PID controllers. Wang et al. \cite{Wang2016ACA} proposes a double layer architecture for a non-linear system using adaptive NN control and Nonlinear Model Predictive Control (NMPC).

\section{Deep Neural Network Security}
%ML security such as adversarial ML
%adding noise to the inputs to change classification but they are based on RNN and other such networks- we use MILP optimization in order to find attacks that cause slight deviations and do not necessarily change the output
%DNNs are susceptible to attacks as is.

Adversarial machine learning emerged when Szegedy et al.  \cite{Szegedy2013IntriguingPO} showed that the input-output mappings for the \ac{DNN}s are not continuous and it is possible to perturb the inputs by small amounts such that the classification changes. 
Since then there has been a lot of work exploring different types of adversarial attacks. Deng et al. \cite{deng2020analysis} analyses five types of adversarial attacks in autonomous driving models. 
ConAML \cite{li2020conaml} explores the constrained adversarial attacks for \ac{CPS} under different settings. They propose a best effort algorithm to iteratively generate adversarial examples but don't consider the physical notion of systems for their attack model. We, on the other hand, introduce \attack  that generate ripples based on constraints. 
Wang et al. \cite{217595} proposes ReLUVal that uses symbolic execution to show that the DNN based systems are free from security vulnerabilities to ensure that in specific intervals, the system is free from attacks; we use MILP that provides us the ability to model different cost functions for different attacks; symbolic execution does not contain this. 
\newline 
\newline 
%Summary
This chapter covers the various techniques that have been used in verification of DNNs, and explains why none of the existing techniques are suitable for our use case. 
We further introduce the various \ac{FDIA} conducted in classical control theory systems and their approaches for demonstrating the attacks. 
We then briefly describe the shift from control theory models to \ac{DNN} based models for \ac{CPS}.
Finally, we explain the security analysis explored in \ac{DNN}s. 