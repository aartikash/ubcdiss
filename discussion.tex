\chapter{Discussion}
We now discuss how our proposed tool might be used to strengthen existing and new DNN-based systems. %\karthik{Did you mean leverage insights ? Also, what're the approaches for ?} 
We also discuss some limitations of our approach.

\section{Strengthening Existing DNNs}
There are many approaches one could use in applying our tool to strenthen existing DNNs. If the original training dataset is available and we can afford to retrain the model, then our tool
could be used to generate adversarial examples based on the current DNN model and these adversarial examples, labeled with the appropriate output, can be added to the training set and the DNN
retrained using this augmented set. This is not unlike how GANs are used to strengthen training for DNNs or how CEGAR loops are used in model-checking. This processes could be repeated for several
iterations until our tool is unable to synthesize attacks either completely, which would be the case if our MILP models are always infeasible, or unable to synthesize attacks within a reasonable timeframe.
The stopping point for this iterative process could be predicated on how safety-critical the DNN application is.

We can also apply the tool if retraining the DNN is not feasible. This would involve generating adversarial examples as described earlier to train a separate DNN that can be coupled
with the existing system to correct its imperfections. This approach seeks to correct deficiencies in the training set after the fact by factoring in input examples that should have been present
and used to train the DNN in the first place. This is effectively similar to the preceding approach except the adversarial examples form their own separate dataset used to train another DNN used for
error correction. This is already done in DNN-based safety critical systems with APS being a specific example that features heavily in this work. The stopping conditions for this iterative approach
can be handled in the same manner as outlined in the previous paragraph.

\section{Strengthening New DNNs}
The problem of using our tool to strengthen new DNNs yet to be trained is interesting. Our tool targets an existing system to generate attacks. It is therefore not possible
to find vulnerabilities in new systems that have yet to be trained. Any such vulnerabilities are clearly dependent on the training set and the methodology used to train the DNN.
Consequently, it is not possible to use our tool in an apriori manner. Therefore, we could use our tool similarly to the case where we are strenghthening an existing DNN that can be feasibly retrained.
The only difference would be the initial step of training the DNN on the existing training set to obtain a DNN to be used as an input for our iterative adversarial example generation strategy.

\section{Limitations}
Integer programming is known to be NP-hard. Solvers are prone to choking as our MILP models become more complex, which is a natural consequence as our tool is applied to larger and more complex DNN systems.
This is a known problem of even state of the art MILP solvers such as Gurobi and CPLEX. However, many complex models do not suffer from this problem and can be solved quickly by these solvers.

Some challenging models can be solved tractably by adding additional constraints to break symmetry between decision variables.
These approaches rely on identifying semantically equivalent decision variables so that ordering constraints on these variables may be added
that lead to elimination of large portions of the state space. It is not immediately obvious how one would identify such inputs for DNNs so how
applicable these are to our approach remains an open question. This has been left as future work.

%\section{Designing Robust DNNs}
%From our experience in working with three different types of DNN based CPS to attack the systems, we realized that there are two ways to construct or build robust DNN based systems. 
%\karthik{Can you say a little bit about how you came to this realization ? Was it based on the experimental results ?}

%\section{Designing Robust DNN from scratch}
%The biggest difference between APS and ACAS Xu apart from the DNN size was the robustness of the DNN. APS had a simple feed-forward architecture, where the network took the inputs, applied the non-linearity to the equations \karthik{non-linearity is not an action !},
% and calculated the outputs. Whereas ACAS Xu applied normalization to its inputs. \karthik{This is a phrase, not a sentence}

%APS had accompanying DNNs that kept\karthik{Duh ?} did a range check to ensure that the output prediction lies within certain bounds \karthik{Did you mean specified bounds ?}. 
%This was easier to break as per our attack model as compared to ACAS Xu\karthik{Can we quantify easier? Also, by break, I guess you mean attack?}. 
%In ACAS Xu, due to the normalization of the inputs, the perturbations had to be carefully designed since one input perturbation did not easily perturb the outputs easily \karthik{Too many easilys!}. 
%We can also observe the percentage of successful attacks from Table II \karthik{Symbolic ref.?} in APS and ACAS Xu. \karthik{What did you observed about them?}
%Hence, we believe that applying techniques while training and modeling DNN architecture can significantly help prevent \attack. \karthik{This is a vacuous statement. What techniques are these ? How does one go about designing such a DNN?}

%\section{Debugging existing DNN using \tool}
%Given there exists a system with DNN similar to that of APS without inbuilt mechanisms, we believe that our tool can help in debugging the DNN by constructing a comprehensive list of cost functions and running the model for different scenarios. This might not provide complete coverage but can certainly help in the falsification process. 
%\karthik{So what's falsification here. Also, it seems like an overly restrictive statement to make about the similarity with APR. What's inbuilt mechanism here ?}

%\section{Limitations}
%There are 2 limitations in our work.
%First, we are assuming that the attacker has access\karthik{Read or write ? Be specific !} to the weights and the bias. However, in a more ideal attack scenario \karthik{for the attacker ?}, if we \karthik{Who's we?} can find a way to  attack the system without knowing the weights and bias of the system, that would be really interesting \karthik{Please avoid weasel words like interesting - make it clear what is of interest}. %Figuring out FDI attacks in that scenario would be challenging yet rewarding. 
%Second, for bigger systems such as ACAS Xu, we observed that running all combinations was taking a lot of time (approx 13 hours), only to tell us that no attack exists \karthik{I suppose us is we as the researcher here. Also, is the time taken a function of the attacks or the network ?}. We think there is scope of improvement to provide speedups by applying clever heuristics to the model. \karthik{Perhaps give 1-2 examples of heuristics here.}


%The limitation of our work is that we do not evaluate the completeness of our approach. We show experimentally that our technique always provides us a solution if the model is properly modeled and if there are attacks possible. Otherwise, it returns an infeasible model.  Since the modeling of the network is represented as a set of well-defined equations it is always going to return a solution if it exists. If no solution exists then it will return the model is infeasible. 
%The interesting part of our approach is that we are not trying to verify but instead we are trying to find feasible FDI attacks for the system. 
\label{section:limitations}