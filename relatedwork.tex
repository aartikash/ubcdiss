\chapter{Related Work}
\label{ch:Chapter2}



We have classified related work into four broad categories. The first category discusses the verification techniques that have emerged to verify DNNs. We highlight why those techniques cannot directly be used for our application. The second discusses work done in the FDIA community for classic control theory modeled systems and why our work is required to understand the consequences of deploying DNNs in CPS.  In the third category, we discuss multiple applications where DNNs have replaced the traditional CPS. This provides a glimpse into how the DNNs are being utilized in safety-critical applications. Furthermore, this motivates the need for a technique that can generalize to different settings of DNN based CPS. Finally, in the last category we discuss the adversarial attacks that have gained a lot of attention in the Machine Learning community and how are they different from our \attack. 
\subsection{Verification of CPS}
%Providing formal guarantees for CPS has always been an important part of the literature. The reason is due to their safety-critical nature. 

%\subsection{Control Theory CPS }
%\subsection{DNN based CPS}
Formally verifying DNN based CPS has recently gained a lot of momentum. Researchers have proposed automated verification mechanisms that use techniques involving SMT solvers, symbolic execution and MILP. However, the modeling that has been done for those interfaces cannot be directly used for our use case without modifications. The goal of these techniques is to conduct an input-output range analysis to establish bounds for the DNN. Our work focuses not on conducting a bound analysis but finding the optimal inputs for changing the outcome of a CPS under a constrained setting for a FDIA. Also, the approaches focus more on verifying certain properties such as reachability analysis that do not account for \attack.

Pulina et al.\cite{10.1007/978-3-642-14295-6_24} proposed one of the first approaches for verifying DNN safety properties using abstraction refinement. They further abstract \cite{article} the nonlinear activation functions in a linear arithmetic SMT formula. They then use counterexample based approach for abstract refinement. There has been further work by Katz et al.\cite{10.1007/978-3-319-63387-9_5} in verifying the networks using SMT solvers where they build rules for handling the ReLU activation function. Most of work we have mentioned above in this domain utilizes the state-of-the art solvers Z3 and CVC4. However, all the existing techniques focus on proving robustness properties \cite{NIPS2016_6339} for DNNs that cannot detect or produce FDIA. Xiang at al.\cite{xiang2017output} focuses on an output reachable estimation for NN using simulation-based methods. There have been multiple methods proposed to conduct a rechability analysis for DNN using MILP/SMT approach \cite{10.1145/3302504.3313351} \cite{ehlers2017formal} \cite{10.1007/978-3-319-63387-9_5} \cite{lomuscio2017approach} \cite{article}. Ivanov et al. \cite{ivanov2018verisig} proposes a verification framework for verifying DNN based controllers by abstracting the problem into a hybrid system verification problem.  
%\subsection*{Cyber-Physical Systems Security}
%Sensor spoofing attacks- multiple types such as gps spoofing, most common ones demonstrated are FDI.

\subsection{False Data Injection Attacks (FDIA) in traditional systems}
%FDI attacks in CPS in different domains. Difference in terms of modelling of systems. Sparse FDI attacks
Attacking the CPS by exploiting the deep integration of the physical-nature and cyber nature of the systems has lead to the emergence of FDIAs. 
There are two main categories of FDI attacks that have been proposed in the literature: random FDIA and targeted FDIA. Random FDIA are generated by randomly changing the inputs to cause changes in the state estimation. Targeted FDIAs are the more interesting ones from an attackers perspective because in this case, they have to synthesize the exact values of inputs to cause a change in the output without triggering alarms. Our work focuses on synthesizing random and targeted FDIAs under specific set of constraints based on the specification documentation. 

In FDI attacks, the attacker's goal is to compromise the sensor inputs to mislead the state estimation process \cite{e3f0020abba24d4389aff937fe8bcdd5}. Liu et al. \cite{10.1145/1952982.1952995} introduced the class of FDIA for electric power grids to introduce arbitrary errors into certain state variables without being detected. Giannini et al. \cite{unknown} generate FDIA under specific setting %\karthik{What does this mean?}
where they introduce new information as constraints in the state estimator under the assumption that the new information is not available to the attacker. This can basically be seen as security by obscurity. These techniques generate FDIA for systems modeled using control theory in order to cause a wrong state estimation without getting detected. Our work looks at the CPS constructed using \ac{DNN}. We try to understand if \ac{FDIA} would work in different constraint setting. Bobba et al. \cite{Bobba2010DetectingFD} show that by protecting a strategically selected set of sensor measurements it is possible to detect the attacks proposed by Liu at al. \cite{10.1145/1952982.1952995}.
% Bai et al. \cite{bai2020aigan} proposes AI-GAN which provides a boost up over previous adversarial examples techniques. In contrast, our work is not focusing on adversarial examples for DNNs but FDIA for DNN based CPS in a constrained environment. 

\subsection{DNN controllers in CPS}
This section focuses on intelligent control primarily due to the introduction of neural networks(NN) in designing the control systems. This subsection discusses how different types of \ac{DNN} are utilized in \ac{CPS} modeling. There are multiple types of \ac{DNN} architectures that have been proposed by \ac{CPS} designers in the recent years. This work is mentioned to provide an intuition of the different models that exist and how none of them have a common means of abstraction or representation.   %As discussed in the Problem Formulation section NN allow the input-output mapping because of the nodes connecting in a network.


Fukuda et al. \cite{170966} show how NN can be utilized in multiple industrial systems. Cong et al. \cite{Cong} proposes a recurrent neural network that is used to create a PID neural network (PIDNN). A proportional–integral–derivative controller is a control loop mechanism employing feedback that is widely used in industrial control systems and a variety of other applications requiring continuously modulated control. This allows the network to converge faster since PIDNN has one hidden layer with 3 neurons that represent K$_p$, K$_i$ and K$_d$ that are used in traditional PID controllers. Wang et al. \cite{Wang2016ACA} proposes a double layer architecture for a non-linear system using adaptive NN control and Nonlinear Model Predictive Control (NMPC). However, none of the mentioned work explores the security affects of replacing the standard controllers with DNNs. Our work specifically looks into understanding the affects of replacing the standard controllers with DNNs and \attack is one of the results.

\subsection{Deep Neural Network Security}
%ML security such as adversarial ML
%adding noise to the inputs to change classification but they are based on RNN and other such networks- we use MILP optimization in order to find attacks that cause slight deviations and do not necessarily change the output
%DNNs are susceptible to attacks as is.

The deep learning security comprises the adversarial examples field which was introduced by Szegedy et al. \cite{Szegedy2013IntriguingPO} where they showed that the input-output mappings for the DNNs are not continuous and it is possible to perturb the inputs by small amounts such that the classification changes. Since then there has been a lot of work exploring different types of adversarial attacks. There are two ways of attacking the \ac{ML} models: data poisoning \cite{DBLP:journals/corr/abs-1903-01666}, and adversarial attacks broadly survey by Papernot et al. \cite{DBLP:journals/corr/PapernotMSW16}. Data poisoning attacks perturb the data during the training phase of the networks whereas adversarial attacks usually act on a trained network by falling the specific input set that creates erroneous perturbations in the output. 



 Deng et al. \cite{deng2020analysis} analyses five types of adversarial attacks in autonomous driving models.  ConAML \cite{li2020conaml} explores the constrained adversarial attacks for cyber-physical systems under different settings. They propose a best effort algorithm to iteratively generate adversarial examples. We on the other hand introduce \attack  that generate ripples based on constraints. We are not trying to find inputs that cause erroneous perturbations due to the deviations but due to the design of the systems. 
  Wang et al. \cite{217595} proposes ReLUVal that uses symbolic execution to show that the DNN based systems are free from security vulnerabilities. They use symbolic execution to ensure that in specific intervals the system is free from attacks whereas we formalize the \ac{CPS} using \ac{MILP} . 

%The goal of our work is to find if a DNN based CPS has ripples and to do so we use MILP whose goal is to find a \attack if it exists.
% \karthik{What about the ReLuVal paper [37] ?}

%\aarti{Add more papers here}
%CPS verification 