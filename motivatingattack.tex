\chapter{Ripple Attacks}
\label{attack}

\section{Motivating system: Artificial Pancreas System}
\label{aps}

We use a DNN based \ac{APS}, closed-loop model, by Dutta et al. \cite{10.1007/978-3-319-99429-1_11}  as our motivating example for illustrating a \ac{RFDIA}. 
The \ac{APS} model we use in this section is also our first evaluation system; it is also the simplest of the three systems in terms of \ac{DNN} complexity described in ~\ref{dnncomplexity}.
A patient relies on  \ac{APS} to correctly determine the next dose of insulin to be injected every $t$ minutes. 

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth, height=0.3\linewidth]{Images/APSDNN}
	\caption[APS DNN]{APS DNN designed by Dutta et. takes in 74 inputs of insulin and glucose. The next layers form connections between the insulin and the glucose to make predictions.[8]}
	\label{fig:apsdnn}
\end{figure}

%How is APS constructed?
The APS architecture is  the same as that discussed in Section ~\ref{apsdnn}. 
We examine the \ac{APS} controller model from Dutta et al., which has a feed-forward architecture with $2$ hidden layers and $50$ neurons each, 74 inputs and $1$ output. 
The DNN for APS takes 32 glucose and insulin values discretized over time as inputs and produces the next insulin values as output as shown in Figure ~\ref{fig:apsdnn}. 
 This implies that the future value is predicted based on the inputs from the previous set of values. 
The insulin and glucose values are collected from the sensors and the actuators. 



%Defense mechanisms
There are two phases in a \ac{DNN}'s life; training and testing phase. 
During the training phase, there are three \ac{DNN}s that are trained in an \ac{APS} to predict the next outcome. 
The first \ac{DNN} is used to predict the next outcome. 
The accompanying two \ac{DNN}s are used as defense mechanisms to prevent the patient from an insulin overdose.  
The two \ac{DNN}s are separate from its main decision making controller. 
These \ac{DNN}s are trained based on the latest 30-day patient data, which is collected by monitoring the patient to understand the standard injections for a patient over time. 

One \ac{DNN} learns a lower threshold on injection amount; the other learns an upper bound on injection amount. 
Hence, for every time of the day, based on prior patient characteristics, the minimum and the maximum values are known. 
However, the system will not detect  an adversary  who manages to always administer the maximum allowed dosage through \ac{RFDIA}. 
Hence, the attacker needs to identify inputs and  perturb them  such that the output administers maximum allowed dosage for every injection,
  while being lower than the upper threshold to avoid detection. 
  We further elaborate this in the next section. 


\section{Attack Model}
The attacker's goal is to manipulate the sensor measurements to corrupt output without triggering alarms as shown in Figure ~\ref{fig:attackmodelphysical}. 
The attacker can use network noise or physical sensor tampering to attack the systems ~\cite{10.1145/3319535.3339815}.
 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Images/Attackmodelphysical}
	\caption{Attack model}
	\label{fig:attackmodelphysical}
\end{figure}

We make the following assumptions in the attack model:
\begin{enumerate}
	\item The attacker knows the precise \ac{DNN}  architecture. This information easy to find, as the architectures are  usually specified in the  documentation. 
	\item  The weights and bias of the \ac{DNN}  are known as well through a read-only access to the system.  
	\item The attacker can modify only the inputs to the model.
	\item The \ac{DNN} contains ReLU as its activation function. 
	%All three systems in this work contain ReLU.
\end{enumerate}

\subsection{Strawman attacker}
A simple way to attack the system would be to change all the 74 inputs to the DNN.
 If all the inputs are changed, the final output prediction is going to be wrong. The problem in this  scenario is that 
 these inputs are collected every five minutes from the sensors attached to the patient. 
 This means that the attacker will have to conduct FDIAs every five minutes to cause a change in the output. 
 However, this is quite tedious in practice, and hence the attacker needs a better way to attack the system. 

\subsection{Sophisticated Attacker}
A more sophisticated approach to attack the system would be by perturbing small number of inputs that cause a change in the output. 
There are two ways to proceed:

\subsubsection{Attack 1}
The attacker can randomly choose two inputs and perturb them by large amounts. 
This will indeed cause a wrong output prediction. 
However, if the input is perturbed by a large amount, the error detection mechanisms will likely recognize that there is an anomaly. 
To prevent this, the attacker can choose to perturb the two inputs by small amounts. 
However, perturbing any two random inputs by small amounts might not necessarily lead to a wrong output prediction, as shown in Chapter ~\ref{evaluation}. 

\subsubsection{Attack 2}
Adding one more layer of sophistication, the attacker carefully chooses the inputs that can lead to wrong predictions. 
 This will be a more targeted approach for attacking the system since the input selection will not be random as before. 
However, not knowing the precise amounts by which the critical inputs should be perturbed can lead to an unsuccessful attack. 
If the perturbation is too high, it will trigger the error-detection mechanisms, 
and if it is too low, it will not affect the output as shown in Chapter ~\ref{evaluation}. 

\subsubsection{Attack 3}
The ideal attack would change a minimal number of inputs by the smallest amount possible to produce the desired change in output. 
We demonstrate that it is possible to generate this kind of attack.
 

%Our motivation for designing the technique were these different ways of attacking the system that can automatically synthesize attacks in an efficient way. 

\input{challenges}













